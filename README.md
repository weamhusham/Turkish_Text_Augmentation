Augmenting Turkish Text Datasets with LLMs and Embedding Models
Project Overview
This project explores the impact of data augmentation and representation learning on machine learning models for text classification and sentiment analysis. By employing advanced embedding models and large language models (LLMs), we augment datasets and analyze the performance of various machine learning algorithms. The project is divided into two main parts:
1. Test Augmentation: Enhancing test data with augmented examples generated by LLMs.
2. Training Augmentation: Expanding training datasets to improve model performance.
Results and analysis can be found in the accompanying article.
Course and Instructor
This project is part of the course Kolektif Ö?renme 2024/1 at the Department of Computer Engineering, Yildiz Technical University, supervised by Prof. Dr. Mehmet Fatih AMASYALI.

Datasets
1. Turkish News Dataset: Kaggle Dataset
o A dataset containing Turkish news headlines categorized by their topics.
2. Turkish Sentiment Analysis Dataset: Hugging Face Dataset
o A dataset for sentiment analysis of Turkish text, labeled for positive, negative, or neutral sentiment.

Embedding Models
1. JinaAI Jina-Embeddings-v3
o A Hugging Face model used to generate dense vector embeddings for text.
2. Intfloat Multilingual E5-Large-Instruct
o A multilingual embedding model designed for instruction-following tasks.
3. YTU-CE-Cosmos Turkish-ColBERT
o A Turkish-specific ColBERT embedding model optimized for efficient similarity search.

Large Language Models (LLMs) for Augmentation
1. vngrs-ai/VBART-XLarge-Paraphrasing
o Used for generating paraphrased text augmentations.
2. vngrs-ai/VBART-Large-Paraphrasing
o A smaller version of VBART-XLarge optimized for paraphrasing.
3. ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1
o A GPT-2-based Turkish model for instruction-based text generation.
Augmentation Strategy:
o Test Augmentation: Each test sample is augmented with 3 and 5 paraphrased examples.
o Training Augmentation: Training datasets are expanded by 2x, 3x, and 5x to study performance improvements.

Machine Learning Algorithms
1. XGBoost
2. LightGBM
3. AdaBoost
4. Random Forest (RF)
5. Support Vector Machine (SVM)

Performance Metrics
1. Accuracy
2. Precision
3. Recall
4. F1-Score

Usage Instructions
To replicate the project and its results, follow these steps:
1. Clone the Repository:
2. git clone <repository-url>
cd <repository-folder>
3. Install Dependencies: Install the required Python libraries using requirements.txt:
pip install -r requirements.txt
4. Run the Scripts: Execute the scripts in the following order:
1. Dataset Examination:
o 1-DatasetsExamination.py: Explore and analyze the original datasets for structure and labeling.
2. Dataset Preparation:
o 2-PreparingDatasets.py: Preprocess the datasets (e.g., splitting, formatting, label mapping) for further use.
3. Test Augmentation (Part A):
o 3-PartA-turkish-VBART-XLarge_TestAugmentation.py: Generate test augmentations using the VBART-XLarge model.
o 4-PartA-turkish-VBART-Large_TestAugmentation.py: Generate test augmentations using the VBART-Large model.
o 5-PartA-turkish-gpt2-large-750m-instruct-v0.1_TestAugmentation.py: Generate test augmentations using the Turkish-GPT2 model.
o 6-PartA-Jina Model Embedding.py: Generate embeddings using the JinaAI model and evaluate performance.
o 7-PartA-E5 Model Embedding.py: Generate embeddings using the E5-Large model and evaluate performance.
o 8-PartA-ColBERT Model Embedding.py: Generate embeddings using the Turkish-ColBERT model and evaluate performance.
4. Training Augmentation (Part B):
o 9-PartB-turkish-VBART-XLarge_TrainAugmentation.py: Generate training augmentations using the VBART-XLarge model.
o 10-PartB-turkish-VBART-Large_TrainAugmentation.py: Generate training augmentations using the VBART-Large model.
o 11-PartB-turkish-gpt2-large-750m-instruct-v0.1_TrainAugmentation.py: Generate training augmentations using the Turkish-GPT2 model.
o 12-PartB-Jina Model Embedding.py: Generate embeddings using the JinaAI model and evaluate performance.
o 13-PartB-E5 Model Embedding.py: Generate embeddings using the E5-Large model and evaluate performance.
o 14-PartB-ColBERT Model Embedding.py: Generate embeddings using the Turkish-ColBERT model and evaluate performance.
5. Analyze Results: After running the scripts, results for test and training augmentation are saved in the specified output directories. These results are used in the accompanying article for analysis and visualization.

Repository Structure
1. Datasets Examination:
o 1-DatasetsExamination.py: Analyzes the structure, distribution, and labels of the datasets.
2. Dataset Preparation:
o 2-PreparingDatasets.py: Prepares the datasets for augmentation and model training.
3. Part A (Test Augmentation):
o Scripts for augmenting test datasets with LLMs and evaluating machine learning models with embeddings.
4. Part B (Training Augmentation):
o Scripts for augmenting training datasets and analyzing performance improvements with embeddings.
5. Results:
o Results for all experiments are saved in the specified output directories.

Acknowledgments
We extend our gratitude to Prof. Dr. Mehmet Fatih AMASYALI for his guidance and support throughout this project.

Contact
For any queries, please contact the contributors via the project repository.

